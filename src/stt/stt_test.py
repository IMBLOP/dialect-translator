import torch
import torchaudio
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import kss  # ë¬¸ì¥ ë¶„ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ & processor
model = WhisperForConditionalGeneration.from_pretrained("D:\PyCharm\dialect_translator\src\stt\gangwon_model")
model.to(device)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.generation_config.forced_decoder_ids = None
model.generation_config._from_model_config = True
processor = WhisperProcessor.from_pretrained("D:\PyCharm\dialect_translator\src\stt\gangwon_model")


def load_audio_chunks(file_path, chunk_sec=30):
    waveform, sample_rate = torchaudio.load(file_path)

    # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜ (ì±„ë„ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)

    # ë¦¬ìƒ˜í”Œë§
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(sample_rate, 16000)
        waveform = resampler(waveform)

    # (1, N) -> (N,) í˜•íƒœë¡œ ë³€í™˜
    waveform = waveform.squeeze()

    chunk_size = 16000 * chunk_sec
    chunks = []

    for i in range(0, len(waveform), chunk_size):
        chunk = waveform[i:i + chunk_size]
        # numpy arrayë¡œ ë³€í™˜
        chunk_np = chunk.numpy()
        chunks.append(chunk_np)

    return chunks


def transcribe_long_audio(file_path):
    audio_chunks = load_audio_chunks(file_path)
    full_transcription = []

    print(f"ğŸ” ì´ {len(audio_chunks)}ê°œì˜ chunkë¡œ ë‚˜ëˆ”")

    for idx, chunk in enumerate(audio_chunks):
        # chunkê°€ numpy arrayì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
        if isinstance(chunk, torch.Tensor):
            chunk = chunk.numpy()

        # processorì— ë‹¨ì¼ ì˜¤ë””ì˜¤ë¡œ ì „ë‹¬ (ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì§€ ì•ŠìŒ)
        inputs = processor(chunk, sampling_rate=16000, return_tensors="pt")
        input_features = inputs.input_features.to(device)

        predicted_ids = model.generate(
            input_features,
            max_new_tokens=440,
            num_beams=5,
            do_sample=False,
            early_stopping=True,
            task="transcribe",
            language="ko"
        )

        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        full_transcription.append(transcription)
        print(f"ğŸ§© chunk {idx + 1}/{len(audio_chunks)} ì™„ë£Œ: {transcription[:50]}...")

    combined_text = " ".join(full_transcription)
    return combined_text


# ì‹¤í–‰
if __name__ == "__main__":
    wav_path = r"D:\PyCharm\dialect_translator\Data\gangwon_voice_data\wav1\1.wav"

    try:
        result = transcribe_long_audio(wav_path)

        print("\nğŸ™ï¸ ì „ì²´ ì¸ì‹ ê²°ê³¼ (ë¬¸ì¥ ë‹¨ìœ„):")
        split_sentences = kss.split_sentences(result)

        for sentence in split_sentences:
            if sentence.strip():  # ë¹ˆ ë¬¸ì¥ ì œì™¸
                print(sentence.strip())

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")