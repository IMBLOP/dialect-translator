import os
from dataclasses import dataclass
from typing import Any, Dict, List, Union

import torch
import torch._dynamo
from datasets import Dataset
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
import evaluate

# â”€â”€ ìµœì í™” ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) TorchDynamo ì»´íŒŒì¼ ë¹„í™œì„±í™” (PyTorch 2.x)
torch._dynamo.disable()

# 2) cuDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ (GPUì—ì„œ ê°€ì¥ ë¹ ë¥¸ ì»¤ë„ ì„ íƒ)
torch.backends.cudnn.benchmark = True

# ğŸ”§ í™˜ê²½ ì„¤ì •
os.environ["WANDB_DISABLED"] = "true"

# ğŸ“¦ ë°ì´í„°ì…‹ ë¡œë”©
CACHE_TRAIN_PATH = "E:/hf_cache/processed/train_ds"
CACHE_EVAL_PATH  = "E:/hf_cache/processed/eval_ds"

print("ğŸ“¦ Loading datasets...")
train_ds = Dataset.load_from_disk(CACHE_TRAIN_PATH)
eval_ds  = Dataset.load_from_disk(CACHE_EVAL_PATH)
print(f"âœ… Loaded: {len(train_ds)} train / {len(eval_ds)} eval samples")

# âœ… Whisper ëª¨ë¸ê³¼ Processor ë¡œë“œ (Multilingual + í•œêµ­ì–´ ì„¤ì •)
processor = WhisperProcessor.from_pretrained(
    "openai/whisper-small", language="Korean", task="transcribe"
)
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

# â€» Gradient Checkpointing ì œê±°: ì†ë„ í–¥ìƒì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
# model.config.use_cache = False
# model.gradient_checkpointing_enable()

# ğŸ§© Collator ì •ì˜ (attention_mask ì œê±° í¬í•¨)
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[list, torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # 1) feature_extractorë¡œ pad â†’ input_features + attention_mask ë°˜í™˜
        input_batch = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_batch, return_tensors="pt")

        # 2) ëª¨ë¸ forwardì— ë¶ˆí•„ìš”í•œ attention_mask ì œê±°
        batch.pop("attention_mask", None)

        # 3) labels padding
        label_batch = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_batch, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # 4) BOS í† í°ë§Œ ìˆëŠ” ì²« ì—´ ì œê±°
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# ğŸ“Š CER metric í•¨ìˆ˜
cer_metric = evaluate.load("cer")
def compute_metrics(p):
    pred_ids  = p.predictions
    label_ids = p.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_strs  = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_strs = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    cer = cer_metric.compute(predictions=pred_strs, references=label_strs)
    return {"cer": cer * 100}

# ğŸ—ï¸ í•™ìŠµ ì„¤ì • (Colabê³¼ ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
training_args = Seq2SeqTrainingArguments(
    output_dir="D:/PyCharm/dialect_translator/src/stt/jeju_model",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=5e-6,
    max_steps=4000,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
    fp16=True,                   # GPUê°€ ìˆì„ ë•Œë§Œ ì¼œì§
    eval_accumulation_steps=4,
    load_best_model_at_end=False,
    report_to="none"
)

# ğŸ§  Trainer ì„¤ì •
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,                # ì „ì²´ eval_ds ì‚¬ìš©
    data_collator=collator,
    compute_metrics=None,                # CER ì‚¬ìš© ì‹œ compute_metrics=compute_metrics
    tokenizer=processor.feature_extractor
)

# ğŸš€ í›ˆë ¨ ì‹œì‘
trainer.train()

# ğŸ’¾ ëª¨ë¸ ì €ì¥
trainer.save_model("D:/PyCharm/dialect_translator/src/stt/jeju_model")
processor.save_pretrained("D:/PyCharm/dialect_translator/src/stt/jeju_model")

print("ğŸ‰ Training complete. Model & processor saved!")
