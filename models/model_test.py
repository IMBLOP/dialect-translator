import torch
from transformers import BartForConditionalGeneration, AutoTokenizer

# ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
MODEL_DIR = r"D:\PyCharm\dialect_translator\models\model03"  # ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì •

# ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = BartForConditionalGeneration.from_pretrained(MODEL_DIR)

# GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def translate_dialect_to_standard(text: str) -> str:
    # padding="max_length"ë¥¼ ì—†ì• ê³  padding=Trueë¡œ ë³€ê²½
    inputs = tokenizer([text], return_tensors="pt", max_length=64, truncation=True, padding=True)
    inputs = {key: val.to(device) for key, val in inputs.items()}  # GPUë¡œ ë³´ë‚´ê¸°

    with torch.no_grad():
        output_ids = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            num_beams=4,
            max_length=64,
            early_stopping=True
        )

    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# CLI ì…ë ¥
if __name__ == "__main__":
    print("ğŸ’¬ ì‚¬íˆ¬ë¦¬ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ í‘œì¤€ì–´ë¡œ ë³€í™˜í•´ë“œë¦½ë‹ˆë‹¤!")
    while True:
        try:
            sentence = input("\nğŸ—£ ì‚¬íˆ¬ë¦¬ ì…ë ¥ (ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„°): ").strip()
            if not sentence:
                print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            result = translate_dialect_to_standard(sentence)
            print("â¡ï¸ í‘œì¤€ì–´ ë²ˆì—­:", result)
        except KeyboardInterrupt:
            print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
